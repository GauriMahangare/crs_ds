{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nGPT2LMHeadModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFGPT2LMHeadModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/gauridhumal/Development Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# or \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\" for larger models\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Sample dialogue corpus\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dialogue_corpus \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHello, how are you?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm good, thank you. How about you?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThat sounds interesting! How does it work?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauridhumal/Development%20Projects/UOL-PROJECTs/CRS/DS/crs_ds/notebooks/03_Models/07_chatGPT_Dialogue.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/Development Projects/UOL-PROJECTs/CRS/DS/crs_ds/.crs-ds-venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1288\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1288\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/Development Projects/UOL-PROJECTs/CRS/DS/crs_ds/.crs-ds-venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1267\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[39m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1267\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m   1269\u001b[0m \u001b[39m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[0;31mImportError\u001b[0m: \nGPT2LMHeadModel requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFGPT2LMHeadModel\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # or \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\" for larger models\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Sample dialogue corpus\n",
    "dialogue_corpus = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I'm good, thank you. How about you?\",\n",
    "    \"Not too bad. What brings you here?\",\n",
    "    \"Just wanted to chat. What are you up to?\",\n",
    "    \"I'm working on a project. How about you?\",\n",
    "    \"I'm reading a book. What kind of project?\",\n",
    "    \"It's a machine learning project. I'm using NLP to generate dialogues.\",\n",
    "    \"That sounds interesting! How does it work?\"\n",
    "]\n",
    "\n",
    "# Tokenize and encode the dialogue corpus\n",
    "input_ids = []\n",
    "for line in dialogue_corpus:\n",
    "    input_ids += tokenizer.encode(line, return_tensors='pt')\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Fine-tune the model on the dialogue corpus\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "for epoch in range(5):\n",
    "    for input_id in input_ids:\n",
    "        loss = model(input_id, labels=input_id)[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "# Function to generate text given a seed text using the fine-tuned model\n",
    "def generate_text(seed_text, model, tokenizer, max_length=50):\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "\n",
    "    # Generate text using the model\n",
    "    output = model.generate(input_ids, max_length=max_length, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Generate dialogue\n",
    "seed_text = \"Hello,\"\n",
    "generated_dialogue = generate_text(seed_text, model, tokenizer)\n",
    "print(generated_dialogue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".crs-ds-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
